from datetime import datetime
import streamlit as st
import matplotlib.pyplot as plt
import os
import yaml
import tempfile
from ultralytics import YOLO
import pandas as pd
import mlflow


def choose_model():
    st.write("### Ch·ªçn m√¥ h√¨nh YOLOv8")

    model_option = st.selectbox(
        "Ch·ªçn model",
        options=["YOLOv8n", "YOLOv8s", "YOLOv8m"],
        index=0
    )

    # Map model option to path
    model_paths = {
        "YOLOv8n": "models/yolov8n.pt",
        "YOLOv8s": "models/yolov8s.pt",
        "YOLOv8m": "models/yolov8m.pt",
    }

    model_path = model_paths[model_option]

    return model_path, model_option


def download_model(model_name):
    """Download a pre-trained YOLOv8 model if it doesn't exist"""
    model_path = f"models/{model_name}.pt"

    # Create models directory if it doesn't exist
    os.makedirs("models", exist_ok=True)

    if not os.path.exists(model_path):
        st.info(f"Downloading {model_name}.pt model...")
        try:
            # Use ultralytics download method
            from ultralytics.utils.downloads import download

            url = f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{model_name}.pt"
            download(url, dir="models/")

            if os.path.exists(model_path):
                st.success(f"‚úÖ Downloaded {model_name}.pt successfully!")
                return True
            else:
                st.error(f"‚ùå Failed to download {model_name}.pt")
                return False
        except Exception as e:
            st.error(f"Error downloading model: {str(e)}")
            return False
    return True  # Model already exists


def train_model():
    st.header("Training Module")

    # Check if dataset exists
    yaml_path = './yolov8_dataset/custom_dataset.yaml'
    if not os.path.exists(yaml_path):
        st.warning(
            "‚ö†Ô∏è Dataset not detected. Please go to the Data tab first to download the dataset.")
        return

    # Ch·ªçn model
    model_path, model_name = choose_model()

    # Check if model exists
    if not os.path.exists(model_path):
        st.warning(f"‚ö†Ô∏è Model file not found: {model_path}")
        st.info("Attempting to download the pre-trained model...")

        # Extract model name from path (e.g., "yolov8n" from "models/yolov8n.pt")
        model_filename = os.path.basename(model_path)
        model_name_only = os.path.splitext(model_filename)[0]

        # Try to download the model
        with st.spinner(f"Downloading {model_name_only}.pt..."):
            if download_model(model_name_only):
                st.success(
                    f"‚úÖ Model {model_name_only} downloaded successfully!")
            else:
                # Provide manual download instructions if automatic download fails
                with st.expander("Manual Download Instructions"):
                    st.code("""
    # Download YOLOv8 models
    mkdir -p models
    wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt -O models/yolov8n.pt
    wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt -O models/yolov8s.pt
    wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt -O models/yolov8m.pt
                    """, language="bash")
                return

    # Upload data config YAML
    # st.write("#### C·∫•u h√¨nh d·ªØ li·ªáu (data.yaml)")
    # yaml_file = st.file_uploader("Upload file data.yaml", type=['yaml', 'yml'])
    # yaml_file = os.path.join(os.path.dirname(
    #     os.path.abspath(__file__)), "yolov8_dataset", "custom_dataset.yaml")
    yaml_file = os.path.abspath("./yolov8_dataset/custom_dataset.yaml")

    # Th√™m input cho ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa dataset
    # st.write("#### ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c dataset")
    dataset_root = os.path.abspath("./yolov8_dataset")

    # if not os.path.exists(dataset_root):
    #     st.warning(
    #         f"Th∆∞ m·ª•c '{dataset_root}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")

    #     # Create directory option
    #     if st.button("Create directory structure"):
    #         try:
    #             # Create base directory
    #             os.makedirs(dataset_root, exist_ok=True)

    #             # Create subdirectories
    #             for split in ['train', 'val', 'test']:
    #                 for subdir in ['images', 'labels']:
    #                     os.makedirs(os.path.join(
    #                         dataset_root, split, subdir), exist_ok=True)

    #             st.success(f"Created directory structure at {dataset_root}")
    #         except Exception as e:
    #             st.error(f"Error creating directories: {str(e)}")

    # Tham s·ªë hu·∫•n luy·ªán
    st.write("#### Tham s·ªë hu·∫•n luy·ªán")
    epochs = st.slider("S·ªë epochs", min_value=1, max_value=100, value=30)
    imgsz = st.slider("K√≠ch th∆∞·ªõc ·∫£nh", min_value=320,
                      max_value=1280, value=640)
    batch_size = st.slider("Batch size", min_value=1, max_value=32, value=16)
    lr = st.number_input("Learning rate", min_value=1e-6,
                         max_value=1.0, value=0.01, format="%.6f")

    # Display selected parameters
    st.write("### Th√¥ng s·ªë ƒë√£ ch·ªçn:")

    # Create a dictionary of parameters
    params = {
        "Model": model_name,
        "Epochs": epochs,
        "Image Size": imgsz,
        "Batch Size": batch_size,
        "Learning Rate": lr,
        # "Dataset Path": dataset_root
    }

    params_df = pd.DataFrame(params.items(), columns=["Tham s·ªë", "Gi√° tr·ªã"])
    st.table(params_df)

    if os.path.exists(yaml_file):
        # ƒê·ªçc n·ªôi dung YAML
        with open(yaml_file, 'r') as f:
            yaml_content = yaml.safe_load(f)

        # Ki·ªÉm tra xem YAML c√≥ ch·ª©a c√°c kh√≥a b·∫Øt bu·ªôc kh√¥ng
        required_keys = ['train', 'val']
        missing_keys = [
            key for key in required_keys if key not in yaml_content]

        if missing_keys:
            st.error(
                f"File YAML thi·∫øu c√°c kh√≥a b·∫Øt bu·ªôc: {', '.join(missing_keys)}. Vui l√≤ng ƒë·∫£m b·∫£o file YAML c√≥ c·∫£ 'train' v√† 'val'.")

            # Hi·ªÉn th·ªã n·ªôi dung YAML hi·ªán t·∫°i
            st.write("N·ªôi dung YAML hi·ªán t·∫°i:")
            st.code(yaml.dump(yaml_content), language="yaml")

            # Cung c·∫•p m·∫´u YAML ƒë√∫ng
            st.write("M·∫´u YAML ƒë√∫ng:")
            sample_yaml = {
                'path': dataset_root,
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'names': {
                    0: 'auto',
                    1: 'bicycle',
                    # ... th√™m c√°c l·ªõp kh√°c
                }
            }
            st.code(yaml.dump(sample_yaml), language="yaml")

            return

        # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi trong YAML
        yaml_content['path'] = dataset_root

        # Ki·ªÉm tra xem c√°c th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        train_path = os.path.join(dataset_root, yaml_content['train'])
        val_path = os.path.join(dataset_root, yaml_content['val'])

        if not os.path.exists(train_path):
            st.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c train: {train_path}")
            return

        if not os.path.exists(val_path):
            st.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c validation: {val_path}")
            return

        # L∆∞u t·∫°m file YAML v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
        temp_yaml = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')
        with open(temp_yaml.name, 'w') as f:
            yaml.dump(yaml_content, f)
        data_cfg = temp_yaml.name

        # # Hi·ªÉn th·ªã n·ªôi dung YAML ƒë√£ c·∫≠p nh·∫≠t
        # st.write("#### N·ªôi dung YAML ƒë√£ c·∫≠p nh·∫≠t:")
        # st.code(yaml.dump(yaml_content), language="yaml")

        # ƒê·∫∑t t√™n run_name theo √Ω m√¨nh + ng√†y th√°ng nƒÉm gi·ªù ph√∫t gi√¢y
        run_name = st.text_input(
            "T√™n run", value=model_name) + f"_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        if st.button("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán üî•"):
            with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh..."):
                # Setup MLflow tracking
                mlflow.set_tracking_uri("./mlruns")

                # B·∫Øt ƒë·∫ßu run MLflow
                with mlflow.start_run(run_name=run_name) as run:
                    st.write(f"MLflow Run ID: {run.info.run_id}")
                    try:
                        # Log parameters
                        mlflow.log_param("model", model_name)
                        mlflow.log_param("epochs", epochs)
                        mlflow.log_param("batch_size", batch_size)
                        mlflow.log_param("learning_rate", lr)
                        mlflow.log_param("image_size", imgsz)
                        mlflow.log_param(
                            "data_config", os.path.basename(data_cfg))
                        mlflow.log_param("dataset_path", dataset_root)

                        # Kh·ªüi t·∫°o model
                        model = YOLO(model_path)

                        # Train v·ªõi run_name ƒë·ªÉ t·∫°o th∆∞ m·ª•c ri√™ng
                        results = model.train(
                            data=data_cfg,
                            epochs=epochs,
                            imgsz=imgsz,
                            batch=batch_size,
                            lr0=lr,
                            project='runs/train',
                            name=run_name,
                            exist_ok=True
                        )

                        # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (s·ª≠ d·ª•ng run_name thay v√¨ model_name)
                        save_dir = os.path.join('runs', 'train', run_name)

                        # ƒê·ªçc file CSV ch·ª©a k·∫øt qu·∫£ hu·∫•n luy·ªán
                        results_csv = os.path.join(save_dir, "results.csv")
                        if os.path.exists(results_csv):
                            df = pd.read_csv(results_csv)
                            st.write("#### Training Metrics")
                            st.dataframe(df)

                            # Plot loss curves
                            fig, (ax1, ax2) = plt.subplots(
                                1, 2, figsize=(12, 5))

                            # Plot losses
                            loss_cols = [
                                col for col in df.columns if "loss" in col.lower()]
                            if loss_cols:
                                for col in loss_cols:
                                    if col in df.columns:
                                        ax1.plot(df.index, df[col], label=col)
                                ax1.set_xlabel("Epoch")
                                ax1.set_ylabel("Loss")
                                ax1.set_title("Training Losses")
                                ax1.legend()
                                ax1.grid(True)

                            # Plot metrics
                            metric_cols = [col for col in df.columns if any(
                                metric in col.lower() for metric in ['map', 'precision', 'recall'])]
                            if metric_cols:
                                for col in metric_cols:
                                    if col in df.columns:
                                        ax2.plot(df.index, df[col], label=col)
                                ax2.set_xlabel("Epoch")
                                ax2.set_ylabel("Metric")
                                ax2.set_title("Validation Metrics")
                                ax2.legend()
                                ax2.grid(True)

                            plt.tight_layout()
                            st.pyplot(fig)

                            # Log metrics per epoch to MLflow
                            for idx, row in df.iterrows():
                                for col in df.columns:
                                    val = row[col]
                                    if isinstance(val, (float, int)) and not pd.isna(val):
                                        # Sanitize metric name - replace invalid characters
                                        sanitized_col = col.replace("(", "_").replace(
                                            ")", "_").replace(" ", "_").replace("/", "_")
                                        mlflow.log_metric(
                                            sanitized_col, float(val), step=idx)
                        else:
                            st.warning(
                                f"Kh√¥ng t√¨m th·∫•y file results.csv t·∫°i: {results_csv}")

                        # Log training artifacts
                        weights_dir = os.path.join(save_dir, "weights")
                        if os.path.exists(weights_dir):
                            # Log best model
                            best_model_path = os.path.join(
                                weights_dir, "best.pt")
                            if os.path.exists(best_model_path):
                                mlflow.log_artifact(
                                    best_model_path, artifact_path="models")
                                st.success(
                                    f"‚úÖ Model saved and logged to MLflow: {best_model_path}")

                                # Save the path to session state for later use
                                st.session_state.trained_model_path = best_model_path
                            else:
                                st.warning("Could not find best.pt model file")

                        # Log confusion matrix if exists
                        confusion_matrix_path = os.path.join(
                            save_dir, "confusion_matrix.png")
                        if os.path.exists(confusion_matrix_path):
                            mlflow.log_artifact(
                                confusion_matrix_path, artifact_path="plots")
                            st.image(confusion_matrix_path,
                                     caption="Confusion Matrix")

                        # Log training curves if exist
                        results_png = os.path.join(save_dir, "results.png")
                        if os.path.exists(results_png):
                            mlflow.log_artifact(
                                results_png, artifact_path="plots")
                            st.image(results_png, caption="Training Results")

                        # Display final metrics
                        if hasattr(results, 'results_dict'):
                            final_metrics = results.results_dict
                            st.write("#### Final Training Results:")
                            for key, value in final_metrics.items():
                                if isinstance(value, (int, float)):
                                    # Sanitize metric name - replace invalid characters
                                    sanitized_key = key.replace("(", "_").replace(
                                        ")", "_").replace(" ", "_").replace("/", "_")
                                    mlflow.log_metric(
                                        f"final_{sanitized_key}", value)
                                    st.write(f"**{key}**: {value:.4f}")

                    except Exception as e:
                        st.error(f"L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
                        st.exception(e)
                    finally:
                        # X√≥a file t·∫°m
                        try:
                            os.unlink(data_cfg)
                        except:
                            pass
    else:
        st.info("Vui l√≤ng upload file data.yaml ƒë·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán.")
